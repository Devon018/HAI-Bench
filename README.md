<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
</head>
<body>

  <h1>ðŸ“˜ HAI-Bench</h1>
  <p><strong>HAI-Bench</strong> is a
 new large-scale benchmark designed to evaluate MLLMsâ€™ ability to understand
 complex human-animal interactions. HAI-Bench comprises 1,614 real-world video
 clips across 250+ unique scenarios. Each video is annotated with high-quality
 temporal, emotional, and causal information using a hybrid pipeline that combines
 advanced vision-language models with manual expert verification. HAI-Bench sup
ports three core tasks: (1) fine-grained visual-text grounding, (2) temporal inference
 of behavior and emotion, and (3) causal chain completion. These tasks evaluate key
 components of embodied AI, such as perceptual alignment, temporal reasoning,
 and causal understanding. </p>

  <h2>ðŸ”— Links</h2>
  <p>
    ðŸ“¦ Dataset: <a href="https://huggingface.co/datasets/Devon018/HAI-Bench" target="_blank">Hugging Face</a>
  </p>

  <h2>ðŸš€ Quick Start</h2>
  <p>Here are the basic steps to use this project:</p>

  <h3>1. Clone the project</h3>
  <pre><code>git clone https://github.com/Devon018/HAI-Bench.git
cd HAI-Bench</code></pre>

<h3>2. Install dataset</h3>
<p>You should create a new path under HAI-Bench and put our dataset in it.</p>
<pre><code>mkdir video</code></pre>
<p>Then download the dataset from <a href="https://huggingface.co/datasets/Devon018/HAI-Bench" target="_blank">Hugging Face</a> and put it in the dataset folder.</p>

  <h3>3. Create and activate conda environment</h3>
  <pre><code>conda create -n haibench python=3.8
conda activate haibench</code></pre>

  <h3>4. Install dependencies</h3>
  <pre><code>pip install -r requirements.txt</code></pre>

  <h3>5. Additional setup (choose one based on your needs):</h3>
  
  <h4>5a. For API-based inference:</h4>
  <p>Configure API keys and endpoints in config.yaml:</p>
  <ul>
    <li>For OpenAI models (e.g., GPT-4o)</li>
    <li>For Qwen models (e.g., Qwen-VL-Max)</li>
    <li>For Claude models (e.g., Claude 3.7 Sonnet)</li>
    <li>For Gemini models (e.g., Gemini 2.5 Flash)</li>
  </ul>
  <p>Edit config.yaml with your API keys and base URLs</p>

  <h4>5b. For local model inference:</h4>
  <ul>
    <li>Install PyTorch 
      <p>Please visit https://pytorch.org/ for pytorch installation instructions.</p>
    </li>
    <li>Install vLLM
      <pre><code>pip install vllm</code></pre>
    </li>
  </ul>

  <h3>6. Switch to evaluation module</h3>
  <pre><code>cd evaluation</code></pre>

  <h3>7. Run evaluation script</h3>
  <pre><code>python evaluation.py MODEL_NAME</code></pre>


  <h2>ðŸ“Š Evaluation Options</h2>
  <p>You can run evaluation by:</p>

  <pre><code>python evaluation.py [-h] [--task_name {task1,task2,task3,all}] 
                      model_name
</code></pre>

  <h3>Arguments:</h3>
  <ul>
    <li><code>model_name</code>: Name of the model to evaluate</li>
    <li><code>--task_name</code>: Specific task to run (default: "all")
      <ul>
        <li><code>task1</code>: Fine-grained visual-text grounding evaluation</li>
        <li><code>task2</code>: Temporal inference evaluation</li>
        <li><code>task3</code>: Causal chain completion evaluation</li>
        <li><code>all</code>: Run all evaluation tasks</li>
      </ul>
    </li>
  </ul>

  <h2>ðŸ“¥ Model Download</h2>
  <p>Please download model files and place them in the <code>models/</code> folder in the project root directory. For example:</p>
  <pre><code>yourproject/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ your_model/
</code></pre>
  <p>Models can be downloaded from the following platforms:</p>
  <ul>
    <li><a href="https://huggingface.co/models" target="_blank">Hugging Face Model Page</a></li>
  </ul>
  
  <h2>ðŸ“« Contact Us</h2>
  <p>If you have any questions, please submit an <a href="https://github.com/Devon018/HAI-Bench/issues" target="_blank">Issue</a> or send an email to huangdihong@sjtu.edu.cn</p>

</body>
</html>
